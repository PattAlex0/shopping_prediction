---
title: "Purchasing Intention Data Science Project"
author: "Alex Patterson"
output: html_document
---

```{r setup, include=FALSE}
### Libraries
library(dplyr)
library(ggplot2)
library(ggcorrplot)
library(tidyr)
library(rsample)
library(tibble)

### Download data
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv")

### Load data
shoppers <- read.csv("online_shoppers_intention.csv")

### Import functions
source("func/viz_dodge.R")
source("func/viz_fill.R")
source("func/tab_cum.R")
source("func/tra_intensity.R")
```

# Predicting who will purchase

## Background
The [*Online Shoppers Purchasing Intention Dataset*](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset) comes from an academic paper titled *Real-time prediction of online shoppers' purchasing intention using multilayer perceptron and LSTM recurrent neural networks*. The dataset contains 12,330 individual page-views — each corresponding to a `different user' — to an unnamed commerical website. Of these page-views, 1,908 (15.5%) resulted in a transaction.

### Features

#### Numerical
- Three variables representing the number of pages visited, corresponding to administrative pages, informational pages, and product-related pages
- Three variables representing the total time (seconds) spend on administrative, informational, and product related pages
- Two variables corresponding to the Bounce Rate and Exit Rate
- The 'average page value of the pages visited'
- How close the visit was to a 'special day'

##### What are bounce and exit rates?
Both the bounce and exit rates are values generated for individual web pages. The bounce is the proportion of visits to this page which end without another request (i.e., they are the *only* page visited by the user). The exit rate is the proportion of visits to the page which represent the end of the user's web session.

### Categorical
- Two variables corresponding to the user's browser and operating system
- The user's 'geographical region'
- How the user arrived at the page
- Whether the user is new, returning, or 'other'
- Whether the visit was on a weekend
- The month

### Question of Interest
Because the company is unnamed, I am ignorant regarding where they are based, what they sell, and who their typical user might be. There are, however, some questions which I think would be of interest to most companies

### Who is the 'average' customer
Where do they come from, and what browser & operating-system combo do they use? While basic, these questions identify how website-development should be managed. If, for example, the majority of users are browsing from their phone, then more resources should probably go into mobile web development. Unfortunately, these variables do not have labels identifying what their numerical codes represent, so I cannot answer this question in an interesting manner. 

#### Website abandonment likelihood
While those with the highest likelihood might be determined, those with a lower likelihood could perhaps be swayed into staying on the website longer

#### Who is going to purchase a product
As discussed in the original paper, knowing this would allow the company to target advertisements more effectively. This could take the form of a 'profile' of the users who buy products and how they differ from the average, or the timeframe in which an average user is more likely to make a purchase. 

## Data preparation

First, I convert the non-binary categorical variables to factors, which is an R object that represents categorical variables. This will make the data easier to work with. 

```{r prep-factor}

### Month
shoppers$Month <- factor(shoppers$Month,
                         levels = c("Feb", "Mar", "May", "June", 
                                     "Jul", "Aug", "Sep", "Oct",
                                     "Nov", "Dec"),
                         labels = c("February", "March", "May",
                                    "June", "July", "August",
                                    "September", "October", "November",
                                    "December"))

### Visitor type
shoppers$VisitorType <- factor(shoppers$VisitorType,
                               levels = c("New_Visitor",
                                          "Returning_Visitor",
                                          "Other"),
                               labels = c("New Visitor",
                                          "Returning Visitor",
                                          "Other"))

### Operating system
shoppers$OperatingSystems <- factor(shoppers$OperatingSystems)

### Browser
shoppers$Browser <- factor(shoppers$Browser)

### Region
shoppers$Region <- factor(shoppers$Region)

### Traffic Type
shoppers$TrafficType <- factor(shoppers$TrafficType)

```

Next, I check the data for missing values and duplicates. While there are no missing values, there are 125 rows which are identical to other rows. In an [online notebook](https://rstudio-pubs-static.s3.amazonaws.com/588410_b71a2f1ad47c4eafa145c424f4fc0faf.html) analysing the same dataset, Yuming Liu said that he/she removed these values. In the original paper, however, the duplicates aren't mentioned, so I assume they were kept. Because these values are just repititions, and because none of them represented page-views resulting in a purchase, I decided to remove them. This left me with 12,205 observations, or 98.99% of the original data.

```{r prep-missing}
### Count number of missing rows
shoppers_missing <- nrow(shoppers[!complete.cases(shoppers), ])

```

```{r prep-duplicate}
### Count number of duplicate rows
shoppers_duplicate <- shoppers[duplicated(shoppers), ]

### Remove duplicates from data
shoppers <- shoppers[!duplicated(shoppers), ]
```

Finally, I converted the data into a tibble format. As explained on the website for the package, the tibble, which is an alternative to the dataframe format in R, helps to keep your data more consistent by preventing name changes and keeping values static.

```{r prep-tibble}
shoppers <- as_tibble(shoppers)

print(shoppers)
```

## Data Inspection

### Administration
Looking at the data, 53.76% of users visited at least 1 administrative page, who then spent an average (median) of 85.56 seconds on these pages.

```{r ins_admin_table}
tab_cum(shoppers$Administrative)
```

```{r ins_admin_viz}
shoppers %>%
    filter(Administrative >= 1) %>%
    ggplot() +
    aes(Administrative_Duration) +
    geom_histogram(bins = 20) +
    labs(X = NULL, y = NULL)
```

### Informational

```{r ins_inf_table}
tab_cum(shoppers$Informational)
```

```{r ins_inf_viz}
shoppers %>%
    filter(Informational >= 1) %>%
    ggplot() +
    aes(Informational_Duration) +
    geom_histogram(bins = 20) +
    labs(X = NULL, y = NULL)
```

### Product Related

99.69% of all sessions involved looking at at least 1 product related webpage

```{r ins_tab_prod}
tab_cum(shoppers$ProductRelated)
```

```{r}
shoppers %>%
    filter(ProductRelated >= 1) %>%
    ggplot() +
    aes(ProductRelated_Duration) +
    geom_histogram() +
    labs(X = NULL, y = NULL) +
    scale_x_continuous()
```

### Impossible Values

There are some observations where the number of pages viewed for a section is at least one, but the total time spent on these pages is 0. This is obviously impossible, so I have removed these observations from the data

```{r}

### Administrative
shoppers <- shoppers[!(shoppers$Administrative >= 1 & shoppers$Administrative_Duration == 0), ]

### Informational
shoppers <- shoppers[!(shoppers$Informational >= 1 & shoppers$Informational_Duration == 0), ]

### Product Related
shoppers <- shoppers[!(shoppers$ProductRelated >= 1 & shoppers$ProductRelated_Duration == 0), ]

```

## Data Splitting
I split the data into two parts. The first is the training dataset, which is the data that I will develop my predictive model on. The second is the testing dataset, which is the data I will evaluate my predictive model on. The purpose of doing this is to prevent over-fitting, which is when a predictive model loses generalisability by developing around the idiosyncracies of a particular dataset.

The code below is an application of example code provided in *Practical Data Science with R*, (pp. 109). It takes 10% of the data for testing, leaving 90% for training

```{r prep-split}

### Set seed
set.seed(1353)

### Create sample group column
shoppers$gp <- runif(nrow(shoppers))

### Extract 10% Testing data
shoppers_test <- subset(shoppers, gp <= 0.1)

### Extract 90% Training data
shoppers_train <- subset(shoppers, gp > 0.1)
```

